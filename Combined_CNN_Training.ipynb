{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Combined CNN Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smhall97/deep_dreaming_music/blob/main/Combined_CNN_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K53sDTzDU5Cj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0166e3df-a432-4f49-ba1b-cc14b9f35bc6"
      },
      "source": [
        "# @title Imports\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# @title Imports\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import torch\n",
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "print(torch.__version__) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.9.0+cu102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0XhepPOW0VI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0da2fb4-61dd-4d61-e6e5-354f252ba7f2"
      },
      "source": [
        "# @title Set device (GPU or CPU)\n",
        "# NMA code\n",
        "# inform the user if the notebook uses GPU or CPU.\n",
        "\n",
        "def set_device():\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device != \"cuda\":\n",
        "    print(\"WARNING: For this notebook to perform best, \"\n",
        "        \"if possible, in the menu under `Runtime` -> \"\n",
        "        \"`Change runtime type.`  select `GPU` \")\n",
        "  else:\n",
        "    print(\"GPU is enabled in this notebook.\")\n",
        "\n",
        "  return device\n",
        "\n",
        "device = set_device()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is enabled in this notebook.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DELQCGVQVB1B"
      },
      "source": [
        "# @title Helper Functions and Loaders\n",
        "\n",
        "def scale_minmax(X):\n",
        "\n",
        "    X_scaled = (X - X.min()) / (X.max() - X.min())\n",
        "\n",
        "    return X_scaled\n",
        "\n",
        "\n",
        "def pickle_loader_mel(file):\n",
        "  with open(file, 'rb') as f:\n",
        "      data = pickle.load(f)\n",
        "      data = np.transpose(data, axes=[1, 2, 0])\n",
        "      data = scale_minmax(data)\n",
        "\n",
        "      zeros = np.zeros(data.shape)\n",
        "\n",
        "      data = np.concatenate((data, data, data), axis=2)\n",
        "\n",
        "  return(data)\n",
        "\n",
        "\n",
        "def pickle_loader_stft_real(file):\n",
        "  with open(file, 'rb') as f:\n",
        "      data = pickle.load(f)\n",
        "      data = np.squeeze(data, axis=0)\n",
        "\n",
        "      #real and imaginary parts are scaled independently\n",
        "      data[:,:,0] = scale_minmax(data[:,:,0])\n",
        "      data[:,:,1] = scale_minmax(data[:,:,1])\n",
        "\n",
        "      real = data[:,:,0].unsqueeze(2)\n",
        "\n",
        "      zeros = np.zeros((data.shape[0],data.shape[1],1))\n",
        "      data = np.concatenate((real, real, real), axis=2)\n",
        "\n",
        "  return(data)\n",
        "\n",
        "\n",
        "def pickle_loader_stft(file):\n",
        "  with open(file, 'rb') as f:\n",
        "      data = pickle.load(f)\n",
        "      data = np.squeeze(data, axis=0)\n",
        "\n",
        "      #real and imaginary parts are scaled independently\n",
        "      data[:,:,0] = scale_minmax(data[:,:,0])\n",
        "      data[:,:,1] = scale_minmax(data[:,:,1])\n",
        "\n",
        "      zeros = np.zeros((data.shape[0],data.shape[1],1))\n",
        "      data = np.concatenate((data, zeros), axis=2)\n",
        "\n",
        "  return(data)\n",
        "\n",
        "\n",
        "def make_sets(classes, items_per_class, ratios):\n",
        "  \"\"\"\n",
        "  parameters:\n",
        "  classes: number of classes in dataset\n",
        "  items_per_class: elements per class (assumes that the dataset is balanced across classes)\n",
        "  ratios: list or array with ratios for each subset [ratio_trainining, ratio_validation, ratio_test]\n",
        "  \"\"\"\n",
        "\n",
        "  train_size = ratios[0] * items_per_class\n",
        "  val_size = ratios[1] * items_per_class\n",
        "  test_size = ratios[2] * items_per_class\n",
        "\n",
        "  test_ix, val_ix, train_ix = np.array([]),np.array([]),np.array([])\n",
        "\n",
        "  for i in range(classes):\n",
        "    class_ix = items_per_class * i\n",
        "\n",
        "    train_ix = np.append(train_ix, np.arange(train_size) + class_ix)\n",
        "    val_ix = np.append(val_ix, np.arange(train_size, train_size + val_size) + class_ix)\n",
        "    test_ix = np.append(test_ix, np.arange(train_size + val_size, train_size + val_size + test_size) + class_ix)\n",
        "\n",
        "  subsets = {\n",
        "          'train': torch.utils.data.Subset(dataset, train_ix.astype(int)),\n",
        "          'val': torch.utils.data.Subset(dataset, val_ix.astype(int)),\n",
        "          'test': torch.utils.data.Subset(dataset, test_ix.astype(int))\n",
        "          }\n",
        "\n",
        "  return subsets\n",
        "\n",
        "\n",
        "def get_cfg_transform(t, augment):\n",
        "\n",
        "  if t == 'stft_r':\n",
        "    params = '{}_{}'.format(str(n_fft), str(hop_length))\n",
        "    pickle_loader = pickle_loader_stft_real\n",
        "    dims = (513, 430)\n",
        "\n",
        "  if t == 'stft_c':\n",
        "    params = '{}_{}'.format(str(n_fft), str(hop_length))\n",
        "    pickle_loader = pickle_loader_stft\n",
        "    dims = (513, 430)\n",
        "\n",
        "  elif t == 'mel':\n",
        "    params = '{}_{}_{}'.format(str(n_fft), str(hop_length), n_mels)\n",
        "    pickle_loader = pickle_loader_mel\n",
        "    dims = (128, 860)\n",
        "  else:\n",
        "    print('ERROR: Unkown transform')\n",
        "\n",
        "  #Set the pytorch transforms for the dataloaders (not the same as the mathematical transforms from before)\n",
        "  if augment:\n",
        "    data_transforms = transforms.Compose([\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.CenterCrop(dims),\n",
        "                                      transforms.Normalize(mean=[.5,.5,.5], std=[.5,.5,.5]),\n",
        "                                      transforms.RandomErasing()\n",
        "                                      ])\n",
        "  else:\n",
        "    data_transforms = transforms.Compose([\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.CenterCrop(dims),\n",
        "                                      transforms.Normalize(mean=[.5,.5,.5], std=[.5,.5,.5])\n",
        "                                      ])\n",
        "\n",
        "\n",
        "\n",
        "  return params, pickle_loader, data_transforms\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7yhYaDrW_Cg"
      },
      "source": [
        "# @title Train model function from PyTorch\n",
        "\n",
        "# Original code from this tutorial: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    train_acc_list, val_acc_list = [], []\n",
        "    # train_loss, validation_loss = [], []\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            num_examples = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                num_examples += inputs.size(0)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs.float())\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "            # Different to tutorial, hardcoded dataset size\n",
        "            # print(dataset_sizes) from above\n",
        "\n",
        "            #if phase == 'train':\n",
        "            #    num_examples = len(train_loader) * len(next(iter(train_loader))[0])\n",
        "            #else:\n",
        "            #    num_examples = len(val_loader) * len(next(iter(val_loader))[0])\n",
        "            print('number of examples in loader = ', num_examples)\n",
        "            print(f'RUNNING LOSS: {running_loss}, RUNNING CORRECTS: {running_corrects}')\n",
        "\n",
        "            epoch_loss = running_loss / num_examples\n",
        "            print()\n",
        "            epoch_acc = running_corrects.double() / num_examples\n",
        "            if phase == 'train':\n",
        "              train_acc_list.append(epoch_acc)\n",
        "            else:\n",
        "              val_acc_list.append(epoch_acc)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    # model = model.to('cuda')\n",
        "    return model, train_acc_list, val_acc_list\n",
        "\n",
        "\n",
        "\n",
        "def train_inception(model, criterion, optimizer, scheduler, n_epochs=25):\n",
        "    # Training Loop\n",
        "    since = time.time()\n",
        "\n",
        "    # Keep track of weights with best loss performance...\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    # Keep track of train-val accuracy stats\n",
        "    train_acc_list, val_acc_list = [], []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f'Epoch {epoch}/{n_epochs-1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # For each epoch, do a training and evaluation (against the validation set) phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            elif phase == 'val':\n",
        "                model.eval()   # Set model to evaluate mode, especially important for dropout layers!\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            num_examples = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                \n",
        "                outputs = None\n",
        "                loss = None\n",
        "                preds = None\n",
        "                maxpreds = None\n",
        "\n",
        "                num_examples += int(inputs.data.shape[0])\n",
        "\n",
        "\n",
        "                inputs = inputs.to(device)\n",
        "                inputs = inputs.float()\n",
        "                labels = labels.to(device)\n",
        "\n",
        "\n",
        "                if phase == 'train':\n",
        "\n",
        "                    # zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # forward\n",
        "                    outputs,_ = model(inputs)\n",
        "                    maxvalues, maxidx = torch.max(outputs.cpu().data, axis=1)\n",
        "    #                 print(\"Output size: \", outputs.shape)\n",
        "    #                 print(\"Labels size: \", labels.shape, labels.data)\n",
        "    #                 print(\"Predictions size: \", maxidx.shape, maxidx.data)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "\n",
        "                elif phase == 'val':\n",
        "\n",
        "                    with torch.inference_mode(True):\n",
        "                        # forward\n",
        "                        outputs = model(inputs)\n",
        "                        maxvalues, maxidx = torch.max(outputs.cpu().data, axis=1)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                # statistics\n",
        "                last_labels = labels.cpu().data\n",
        "                last_preds = maxidx.data\n",
        "                running_loss += float(loss.cpu().data) * inputs.cpu().data.shape[0]\n",
        "                running_corrects += torch.sum(last_preds == last_labels)\n",
        "\n",
        "                if num_examples % 100 == 0:\n",
        "                    print(f' ..{phase}-{num_examples}.. ', end='')\n",
        "\n",
        "                # Explicit memory cleanup\n",
        "                del inputs\n",
        "                del labels\n",
        "                del outputs\n",
        "                del loss\n",
        "                del preds\n",
        "                del maxpreds\n",
        "\n",
        "            print(\"Predicted  : \", last_preds)\n",
        "            print(\"Real Labels: \", last_labels)\n",
        "            print('number of examples trained on = ', num_examples)\n",
        "            print(f'RUNNING LOSS: {running_loss}, RUNNING CORRECT PREDS: {running_corrects}')\n",
        "\n",
        "            epoch_loss = running_loss / num_examples\n",
        "            print()\n",
        "            epoch_acc = running_corrects.double() / num_examples\n",
        "\n",
        "            if phase == 'train':\n",
        "                train_acc_list.append(epoch_acc)\n",
        "            elif phase == 'val':\n",
        "                val_acc_list.append(epoch_acc)\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # A new record for best accuracy on the validation set\n",
        "            #  Keep a record of the weights and the new epoch accuracy\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    # model = model.to('cuda')\n",
        "    return model, train_acc_list, val_acc_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdgBbct-Wjr1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49368045-d799-4eef-f4ff-2a34ba8817d4"
      },
      "source": [
        "# @title Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive') #it will ask you for a verification code"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kwTJL1OVWn9"
      },
      "source": [
        "# @title Parameters\n",
        "\n",
        "#general paramenters\n",
        "mount_drive = True\n",
        "models_list = ['vgg16', 'inception3']\n",
        "transforms_list = ['stft_c', 'stft_r', 'mel'] #transforms to be trained on. Takes 'mel', 'stft' or 'stft_r'\n",
        "path = '/content/drive/MyDrive/HallucinatingGANs/Code/data/'\n",
        "modelpath = '/content/drive/MyDrive/HallucinatingGANs/Code/data/models/'\n",
        "outpath = '/content/drive/MyDrive/HallucinatingGANs/Code/data/models/finetuned/'\n",
        "\n",
        "#audio transforms parrameters\n",
        "n_fft = 1024\n",
        "n_mels = 128\n",
        "hop_length = 256 # smaller hop size leads to better reconstruction but takes longer to compute\n",
        "power = 2.0 # squared power spectrogram\n",
        "samplerate =  22050\n",
        "\n",
        "#model training parameters\n",
        "epochs = 40\n",
        "n_workers = 4\n",
        "minibatch_size = 20\n",
        "finetune = True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASeF_oqHhALU"
      },
      "source": [
        "# @title Main Loop\n",
        "\n",
        "for transf in transforms_list:\n",
        "  for augment in [False, True]:\n",
        "    for model_name in models_list:\n",
        "\n",
        "      #get settings for the transform to be performed\n",
        "      params, pickle_loader, data_transforms = get_cfg_transform(transf, augment)\n",
        "   \n",
        "      if model_name == 'inception3' and transf == 'mel':\n",
        "          data_transforms.transforms.insert(-1,transforms.Resize((299,860)))\n",
        "\n",
        "      #names of ouput files\n",
        "      if augment:\n",
        "          label = '{}AUG_{}_{}'.format(model_name, transf, params)\n",
        "      else:\n",
        "          label = '{}_{}_{}'.format(model_name, transf, params)\n",
        "\n",
        "      acc_file = outpath + label + '.pkl'\n",
        "      model_file = outpath + label + '.pt'\n",
        "      print(label)\n",
        "\n",
        "      if transf[:4] == 'stft':\n",
        "          transf = 'stft'\n",
        "\n",
        "      #path of the corresponding spectrograms\n",
        "      data_dir = os.path.join(os.path.abspath(path), 'spectrograms', transf, params)\n",
        "      print(data_dir)\n",
        "\n",
        "      #load dataset\n",
        "      dataset = torchvision.datasets.DatasetFolder(root=data_dir,\n",
        "                                                transform = data_transforms,\n",
        "                                                loader=pickle_loader,\n",
        "                                                extensions='.pkl',\n",
        "                                                )\n",
        "      #get genres and number of classes in the dataset\n",
        "      genres = list(os.listdir(data_dir))\n",
        "      n_classes = len(genres)\n",
        "\n",
        "      #generate training, validation and test sets\n",
        "      subsets = make_sets(classes=n_classes,\n",
        "                                          items_per_class=100,\n",
        "                                          ratios=[.8, .1, .1])\n",
        "\n",
        "      #create dataloaders\n",
        "      dataloaders = {x: torch.utils.data.DataLoader(subsets[x], batch_size=minibatch_size,\n",
        "                                                shuffle=True, num_workers=n_workers)\n",
        "                  for x in ['train', 'val']}\n",
        "\n",
        "      dataset_sizes = {x: len(dataloaders[x].dataset) for x in ['train', 'val']}\n",
        "      print(dataset_sizes)\n",
        "\n",
        "      if model_name == 'vgg16':\n",
        "          net = models.vgg16(pretrained=True)\n",
        "          #net = torch.load(modelpath + 'vgg16.pth')\n",
        "\n",
        "          # unfreeze layers to allow finetuning\n",
        "          for param in net.parameters():\n",
        "            param.requires_grad = finetune # If True it will train\n",
        "\n",
        "          # Parameters of newly constructed modules have requires_grad=True by default\n",
        "          # Add on classifier\n",
        "          net.classifier[6] = nn.Sequential(\n",
        "                              nn.Linear(net.classifier[3].in_features, 256),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Linear(256, n_classes),\n",
        "                              nn.LogSoftmax(dim=1))\n",
        "\n",
        "      elif model_name == 'inception3':\n",
        "          net = models.inception_v3(pretrained=True)\n",
        "          #net = torch.load(modelpath + 'inception3.pth')\n",
        "\n",
        "          # unfreeze layers to allow finetuning\n",
        "          for param in net.parameters():\n",
        "              param.requires_grad = finetune\n",
        "\n",
        "          # Add our own classifier layer, replacing the ImageNet classifier, on to the end of inception3\n",
        "          net.fc = nn.Sequential(\n",
        "              nn.Linear(2048, 256, bias=True),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(256, n_classes, bias=True),\n",
        "              nn.LogSoftmax(dim=1))\n",
        "\n",
        "\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      optimizer_conv = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "      exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1) # Decay LR by a factor of 0.1 every 7 epochs\n",
        "\n",
        "      # model trains - change num_epochs to increase training time\n",
        "      net = net.float()\n",
        "      if model_name != 'inception3':\n",
        "          model_ft, train_acc_list, val_acc_list = train_model(net.to(device), criterion, optimizer_conv, exp_lr_scheduler,\n",
        "                              num_epochs=epochs)\n",
        "      else:\n",
        "          model_ft, train_acc_list, val_acc_list = train_inception(net.to(device), criterion, optimizer_conv, exp_lr_scheduler,\n",
        "                              n_epochs=epochs)\n",
        "          \n",
        "      #save accuracies from training procedure\n",
        "      acc_dict = {'train_acc': train_acc_list, 'val_acc': val_acc_list}\n",
        "      print('writing file: ' + acc_file)\n",
        "      with open(acc_file, 'wb') as f:\n",
        "        pickle.dump(acc_dict, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "      #save model\n",
        "      torch.save(model_ft.state_dict(), model_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMyQ33jc0lm7"
      },
      "source": [
        "## Check classifications for each genre in the validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83BcX9Tt0kPg"
      },
      "source": [
        "dataloaders_val = torch.utils.data.DataLoader(subsets['val'], batch_size=1,\n",
        "                                             shuffle=False, num_workers=1)\n",
        "model_ft.eval()\n",
        "out_genres = {}\n",
        "for i in range(10):\n",
        "  out_genres[i] = []\n",
        "\n",
        "for item in iter(dataloaders_val):\n",
        "  out_temp = model_ft(item[0].to(device))\n",
        "  out_numpy = out_temp[0].to(\"cpu\").detach().numpy()\n",
        "  max_ind_out = out_numpy.argmax()\n",
        "  out_genres[item[1].numpy()[0]].append(max_ind_out)\n",
        "  #print(item[1].numpy()[0])\n",
        "print(out_genres)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}